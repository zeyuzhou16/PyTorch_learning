{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aafd698b",
   "metadata": {},
   "source": [
    "The goal of this file is to implement bagging, boosting and stacking almost from scratch to fully appreciate their differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10bf3fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard packages\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Base parent classes for bagging\n",
    "from sklearn.base import clone, BaseEstimator, ClassifierMixin\n",
    "\n",
    "# Data engineering for demo\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Base_estimators for bagging and boosting\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Heterogeneous Learners for stacking\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d07d6f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBag(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_estimator = None, n_estimators = 10, subset_size = 0.8):\n",
    "        self.base_estimator = base_estimator if base_estimator else DecisionTreeClassifier(max_depth=1, max_features=1)\n",
    "        self.n_estimators = n_estimators\n",
    "        self.subset_size = subset_size\n",
    "        self.base_learners = []\n",
    "        self.is_fitted = False\n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        subset_size = int(n_samples * self.subset_size)\n",
    "        self.base_learners = []\n",
    "        # This is the key step\n",
    "        # Each estimator get a subset and train\n",
    "        # Use a list to save all basic learners.\n",
    "        for _ in range(self.n_estimators):\n",
    "            idx = np.random.choice(range(n_samples), size=subset_size, replace=True)\n",
    "            X_subset, y_subset = X[idx], y[idx]\n",
    "            cloned_estimator = clone(self.base_estimator)\n",
    "            cloned_estimator.fit(X_subset, y_subset)\n",
    "            self.base_learners.append(cloned_estimator)\n",
    "        self.is_fitted = True\n",
    "    def predict(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise Exception(\"This simplebag is not fitted/trained yet\")\n",
    "        else:\n",
    "            predictions = np.array([clf.predict(X) for clf in self.base_learners]).T\n",
    "            final_predictions, _ = mode(predictions, axis=1)\n",
    "            return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc43048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleMultiClassBoosting(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_estimator = DecisionTreeClassifier(max_depth=1), n_estimators = 50):\n",
    "        super().__init__()\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learners = []\n",
    "        self.learner_weights = []\n",
    "        self.label_encoder = LabelEncoder()\n",
    "    def fit(self, X, y):\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        self.n_classes = len(self.label_encoder.classes_)\n",
    "        n_sample = X.shape[0]\n",
    "        sample_weights = np.ones(n_sample) / n_sample\n",
    "        for _ in range(self.n_estimators):\n",
    "            learner = clone(self.base_estimator)\n",
    "            learner.fit(X, y_encoded, sample_weight = sample_weights)\n",
    "            learner_pred = learner.predict(X)\n",
    "            incorrect = (learner_pred != y_encoded)\n",
    "            learner_error = np.mean(np.average(incorrect, weights=sample_weights))\n",
    "            learner_weight = np.log((1-learner_error) / (learner_error + 1e-10)) + np.log(self.n_classes - 1)\n",
    "            if learner_error >= 1 - 1/self.n_classes:\n",
    "                break\n",
    "            # Increase the weights of misclassified samples\n",
    "            sample_weights *= np.exp(learner_weight * incorrect * (sample_weights > 0))\n",
    "            sample_weights /= np.sum(sample_weights)  # Normalize weights\n",
    "            \n",
    "            # Save the current learner\n",
    "            self.learners.append(learner)\n",
    "            self.learner_weights.append(learner_weight)\n",
    "        self.n_finalized_estimators = len(self.learners)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Collect predictions from each learner\n",
    "        learner_preds = np.array([learner.predict(X) for learner in self.learners])\n",
    "        \n",
    "        # Weighted vote for each sample's prediction across all learners\n",
    "        weighted_preds = np.zeros((X.shape[0], len(self.label_encoder.classes_)))\n",
    "        for i in range(len(self.learners)):\n",
    "            weighted_preds[np.arange(X.shape[0]), learner_preds[i]] += self.learner_weights[i]\n",
    "        \n",
    "        # Final prediction is the one with the highest weighted vote\n",
    "        y_pred = np.argmax(weighted_preds, axis=1)\n",
    "        # Convert back to original class labels\n",
    "        return self.label_encoder.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f21629a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleStacking(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_learners: list, meta_learner=SVC(probability=True, random_state=42)):\n",
    "        self.base_learners = base_learners\n",
    "        self.meta_learner = meta_learner\n",
    "        self.fitted_base_learners = []\n",
    "    def fit(self, X, y):\n",
    "        meta_features = []\n",
    "        self.fitted_base_learners = []\n",
    "\n",
    "        for lnr in self.base_learners:\n",
    "            fitted_lnr = clone(lnr).fit(X, y)\n",
    "            self.fitted_base_learners.append(fitted_lnr)\n",
    "            preds = fitted_lnr.predict(X)\n",
    "            meta_features.append(preds)\n",
    "        meta_features = np.array(meta_features).T\n",
    "        self.meta_learner.fit(meta_features, y)\n",
    "    def predict(self, X):\n",
    "        meta_features = [lrn.predict(X) for lrn in self.fitted_base_learners]\n",
    "        meta_features = np.array(meta_features).T\n",
    "        return self.meta_learner.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7899df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 4)\n",
      "Accuracy of the single Decision Tree model: 0.87\n",
      "Accuracy of the SimpleBag ensemble model: 0.73\n",
      "Accuracy of the SimpleBoost ensemble model: 1.00\n",
      "Accuracy of the SimpleStack ensemble model: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_test.shape)\n",
    "# Train and evaluate a single Decision Tree\n",
    "single_tree = DecisionTreeClassifier(max_depth=2, max_features=1)\n",
    "single_tree.fit(X_train, y_train)\n",
    "# single_tree_predictions = single_tree.predict(X_test)\n",
    "# single_tree_accuracy = accuracy_score(y_test, single_tree_predictions)\n",
    "single_tree_accuracy = single_tree.score(X_test, y_test)\n",
    "\n",
    "# Initialize, fit, and evaluate the SimpleBag model\n",
    "simple_bag = SimpleBag(n_estimators=100, subset_size=0.5)\n",
    "simple_bag.fit(X_train, y_train)\n",
    "# simple_bag_predictions = simple_bag.predict(X_test)\n",
    "#simple_bag_accuracy = accuracy_score(y_test, simple_bag_predictions)\n",
    "simple_bag_accuracy = simple_bag.score(X_test, y_test)\n",
    "\n",
    "simple_boost = simpleMultiClassBoosting(n_estimators=10)\n",
    "simple_boost.fit(X_train, y_train)\n",
    "# simple_bag_predictions = simple_bag.predict(X_test)\n",
    "#simple_bag_accuracy = accuracy_score(y_test, simple_bag_predictions)\n",
    "simple_boost_accuracy = simple_boost.score(X_test, y_test)\n",
    "\n",
    "simple_stack = simpleStacking([DecisionTreeClassifier(max_depth=1, max_features=1), LogisticRegression(random_state=42)], SVC(probability=True, random_state=42))\n",
    "simple_stack.fit(X_train, y_train)\n",
    "simple_stack_accuracy = simple_stack.score(X_test, y_test)\n",
    "\n",
    "\n",
    "print(f'Accuracy of the single Decision Tree model: {single_tree_accuracy:.2f}')\n",
    "print(f'Accuracy of the SimpleBag ensemble model: {simple_bag_accuracy:.2f}')\n",
    "print(f'Accuracy of the SimpleBoost ensemble model: {simple_boost_accuracy:.2f}')\n",
    "print(f'Accuracy of the SimpleStack ensemble model: {simple_stack_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3537275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of One Single Decision Tree model: 0.87\n",
      "Accuracy of the sklearn baggingclassifier model: 1.00\n",
      "Accuracy of the sklearn boostingclassifier model: 0.93\n",
      "Accuracy of the sklearn stackingclassifier model: 1.00\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy of One Single Decision Tree model: {single_tree_accuracy:.2f}')\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging_model = BaggingClassifier(estimator = DecisionTreeClassifier(max_depth=2, max_features=1), n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "# predictions = bagging_model.predict(X_test)\n",
    "# acc = accuracy_score(y_test, predictions)\n",
    "bg_acc = bagging_model.score(X_test, y_test)\n",
    "print(f'Accuracy of the sklearn baggingclassifier model: {bg_acc:.2f}')\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "boosting_model = AdaBoostClassifier(estimator = DecisionTreeClassifier(max_depth=1), n_estimators=100, random_state=42, algorithm = 'SAMME')\n",
    "boosting_model.fit(X_train, y_train)\n",
    "# predictions = bagging_model.predict(X_test)\n",
    "# acc = accuracy_score(y_test, predictions)\n",
    "bst_acc = boosting_model.score(X_test, y_test)\n",
    "print(f'Accuracy of the sklearn boostingclassifier model: {bst_acc:.2f}')\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "stacking_model = StackingClassifier(estimators = [('decision_tree', DecisionTreeClassifier(max_depth=1)), ('lr', LogisticRegression())], final_estimator=SVC(probability=True, random_state=42), cv=5)\n",
    "stacking_model.fit(X_train, y_train)\n",
    "# predictions = bagging_model.predict(X_test)\n",
    "# acc = accuracy_score(y_test, predictions)\n",
    "stk_acc = stacking_model.score(X_test, y_test)\n",
    "print(f'Accuracy of the sklearn stackingclassifier model: {stk_acc:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
