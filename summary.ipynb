{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Preprocessing:\n",
    "    1.  a. from sklearn.preprocessing import StandardScaler\n",
    "            sc = StandardScaler()\n",
    "            x_train_std = sc.fit_transform(x_train)\n",
    "        b. from sklearn.preprocessing import MinMaxScaler\n",
    "            mms = MinMaxScaler()\n",
    "            x_train_norm = sc.fit_transform(x_train)\n",
    "    2a. from sklearn.impute import SimpleImputer\n",
    "        imr = SimpleImputer(missing_values = np.nan, strategy = 'mean/median/most_frequent/constant', fill_value = 0, copy = True)\n",
    "        x_train_std = imr.fit_transform(x_train)\n",
    "    2b. from sklearn.impute import KNNImputer\n",
    "        imputer = KNNImputer(n_neighbors = 3, weights = 'uniform/distance', metric = 'nan_euclidean', copy = False)\n",
    "        imputer.fit_transform(x_train)\n",
    "    3. Encoding class labels\n",
    "        3a. pd.DataSeries.map(dict or lambda func)\n",
    "        3b. from sklearning preprocessing import LabelEncoder\n",
    "            color_le = LabelEncoder()\n",
    "            y = color_le.fit_transform(y_train)\n",
    "            color_le.inverse_transform(y)\n",
    "        3c. from sklearn.preprocessing import OneHotEncoder\n",
    "            color_ohe = OneHotEncoder(categories = 'auto', drop = 'first')\n",
    "            color_ohe.fit_transform(X[:, 0].reshape(-1, 1)).toarray()\n",
    "            or\n",
    "        3c. from sklearn.compose import ColumnTransformer\n",
    "            c_transf = ColumnTransformer([\n",
    "                ('onehot', OneHotEncoder(), [0]), \n",
    "                ('nothing', 'passthrough', [1, 2])\n",
    "            ])\n",
    "            c_transf.fit_transform(X).astype(np.float32)\n",
    "        3d. pd.get_dummies(df[['price', 'color', 'size]], drop_first = True)    \n",
    "\n",
    "B. Classifiers:\n",
    "    Linear Classifiers:\n",
    "    1. from sklearn.linear_model import Perception\n",
    "    ppn = Perceptron(eta0 = 0.1, random_state = 1)\n",
    "\n",
    "    2. from sklearn.linearmodel import LogisticRegression\n",
    "    lr = LogisticRegression(C = 1.0, solver = 'lbfgs', multi_class = 'ovr/multinominal')\n",
    "        C: inverse of L2 regularization strength\n",
    "    lr = LogisticRegression(penalty = 'l1', C = 1.0, solver = 'liblinear', multi_class = 'ovr/multinominal')\n",
    "        C: inverse of L1 regularization strength\n",
    "    lr.intercept_\n",
    "    lr.coef_\n",
    "\n",
    "    3. from sklearn.svm import SVC\n",
    "    svm = SVC(kernel = 'linear', C = 1.0, random_state = 1)\n",
    "\n",
    "    SGD version of Linear Classifiers:\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    1. ppn = SGDClassifier(loss = 'perceptron')\n",
    "    2. lr = SGDClassifier(loss = 'log')\n",
    "    3. svm = SGDClassifier(loss = 'hinge')\n",
    "\n",
    "    Nonlinear classifiers: (2., 3. don't care about scaling of X_train)\n",
    "    1. svm = SVC(kernel = 'rbf', C = 10.0, random_state = 1, gamma = 0.10)\n",
    "        \\kappa(x^(i), x^(j)) = exp(-gamma ||x^(i) - x^(j)||^2)\n",
    "    2. from sklearn.tree import DecisionTreeClassifier\n",
    "        tree_model = DecisionTreeClassifier(criterion = 'gini/entropy/log_loss', max_depth = 4, random_state = 1)\n",
    "    3. from sklearn.ensemble import RandomForestClassifier\n",
    "        forest = RandomForestClassifier(n_estimator = 25, n_jobs = -1, random_state = 1)\n",
    "    4. from sklearn.neighbors import KNeighborsClassifier\n",
    "    knn = KNeighborsClassifier(n_neighbors = 5, p = 2, metric= 'minkowski')\n",
    "\n",
    "    Classifier-methods:\n",
    "    ppn.fit(x_train_std, y_train)\n",
    "    y_pred = ppn.predict(x_test_std)\n",
    "\n",
    "C. Scores:\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    accuracy_score(y_test, y_pred)\n",
    "\n",
    "    ppn.score(x_test_std, y_test)\n",
    "\n",
    "D. Feature Selections:\n",
    "    1. forest.fit(X_train, y_train)\n",
    "        importances = forest.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1] (descending)\n",
    "\n",
    "    2. from sklearn.feature_selection import SelectFromModel\n",
    "        sfm = SelecFromModel(forest, threshold = 0.1, predit = True)\n",
    "        X_selected = sfm.transform(X_train)\n",
    "\n",
    "E. Dimension Reduction:\n",
    "    1. Unsupervised data compression: principal component analysis\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components = 2)\n",
    "        X_train_pca = pca.fit_transform(X_train_std)\n",
    "    2. Supervised data compression linear discriminant analysis\n",
    "        from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "        lda = LDA(n_components = 2)\n",
    "        X_train_lda = lda.fit_transform(X_train_std, y_train)\n",
    "    3. Nonlinear DR:\n",
    "        from sklearn.manifold import TSNE\n",
    "        tsne = TSNE(n_components = 2, init = 'pca, random_state = 123)\n",
    "        X_train_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "F. Pipeline, Cross Validation and confusion matrix:\n",
    "    1. from sklearn.pipeline import make_pipeline\n",
    "        pipe_lr = make_pipeline(sc, pca, lr)\n",
    "        pipe_lr.fit(X_train, y_train)\n",
    "        y_pred = pipe_lr.predict(X_test)\n",
    "        test_acc = pipe_lr.score(X_test, y_test)\n",
    "    2. sklearn.model_selection\n",
    "        a. train -- test splitting\n",
    "            from sklearn.model_selection import train_test_split <- Simplest splitting\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify = y, random_state = 1)\n",
    "        b. K fold cross-validation splitting train set\n",
    "            from sklearn.model_selection import StratifiedKfold\n",
    "            kfold = StratifiedKfold(n_splits = 10).split(X_train, y_train)\n",
    "            scores = []\n",
    "            for k, (train, test) in enumerate(kfold):\n",
    "                pipe_lr.fit(X_train[train], y_train[train])\n",
    "                scores.append(pipe_lr.score(X_train[test], y_train[test]))\n",
    " \n",
    "        \n",
    "            simpler:\n",
    "            from sklearn.model_selection import cross_val_score\n",
    "            scores = cross_val_score(estimator = pipe_lr, X = X_train, y = y_train, cv = 10, n_jobs = 1)\n",
    "\n",
    "            mean_acc = np.mean(scores)\n",
    "            std_acc = np.std(scores)\n",
    "            print(f'CV accuracy: {np.mean(scores):.3f} +/- {np.std(scores):.3f}')\n",
    "    3. learning_curve\n",
    "        from sklearn.model_selection import learning_curve\n",
    "        pipe_lr = make_pipeline(StandardScaler(), LogisticRegression(penalty='l2', max_iter=10000))\n",
    "        train_sizes, train_scores, test_scores = learning_curve(estimator=pipe_lr, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=10, n_jobs=1)\n",
    "    4. validation_curve\n",
    "        from sklearn.model_selection import validation_curve\n",
    "        param_range = 10.0 ** (np.arange(-3, 3, 1))\n",
    "        train_scores, test_scores = validation_curve(estimator=pipe_lr, X = X_train, y = y_train, param_name='logisticregression__C', param_range=param_range.tolist(), cv = 10)\n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        test_mean = np.mean(test_scores, axis=1)\n",
    "        test_std = np.std(test_scores, axis=1)\n",
    "    5. GridSearchCV\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        from sklearn.model_selection import RandomizedSearchCV\n",
    "        \n",
    "        from sklearn.experimental import enable_halving_search_cv\n",
    "        from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "        param_grid = [{'svc__C':param_range, 'svc__kernel':['linear']}, {'svc__C':param_range, 'svc__gamma':param_range, 'svc__kernel':['rbf']}]\n",
    "\n",
    "        gs = RandomizedSearchCV(estimator=pipe_svc, param_distributions=param_grid, scoring='accuracy', cv = 10, refit = True, n_jobs = -1)\n",
    "\n",
    "    6. More on accuracy\n",
    "        We have exhausted 'accuracy' as the scoring parameter, what other performance evaluation metrics can we use?\n",
    "        Confusion matrix: TP + FN + FP + TN = TOTAL\n",
    "        Accuracy (ACC): (TP + TN) / TOTAL\n",
    "        Error (ERR): (FP + FN) / TOTAL\n",
    "        Recall (REC)/TP Rate : TP / (TP + FN) = TP / P\n",
    "        False Positive Rate : FP / (TN + FP) = FP / N\n",
    "        Precision (PRE) : TP / (TP + FP)\n",
    "        F1 Score : 2PRE * REC / (PRE + REC)\n",
    "        Matthews Correlation coefficient MCC : (TP * TN - FP * FN) / sqrt{(TP + FP)(TN + FP)(TP + FN)(TN + FN)}\n",
    "        Receiver Operating Characteristic (ROC) graphs\n",
    "\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "        from sklearn.metrics import make_scorer\n",
    "        scorer = make_scorer(f1_score, pos_label = 0)\n",
    "\n",
    "    7. Imbalanced Dataset\n",
    "        Class Imbalances:\n",
    "\n",
    "        X_imb = np.vstack((X[y == 0], X[y == 1][:40]))\n",
    "        y_imb = np.hstack((y[y == 0], y[y == 1][:40]))\n",
    "\n",
    "        If 90% of the dataset belong to one category, the precision score is not necessarily the optimal criterion for learning. Moreover, this imbalance leads to suboptimal learning progression.\n",
    "        Solution #1: activate class_weighted = 'balanced' (or assign a predefined dictionary {0:0.9, 1:0.1, ... })\n",
    "        Solution #2: Change the sampling with sklearn.utils.resample\n",
    "            from sklearn.utils import resample\n",
    "            X_upsampled, y_upsampled = resample(X_imb[y_imb == 1], y_imb[y_imb == 1], replace=True, n_samples=X_imb[y_imb == 0].shape[0], random_state=123)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
